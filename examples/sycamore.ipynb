{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_circuits import QuantumCircuit\n",
    "from artensor import (\n",
    "    AbstractTensorNetwork, \n",
    "    ContractionTree, \n",
    "    find_order, \n",
    "    tensor_contraction_einsum, \n",
    "    contraction_scheme_sparse_einsum_1\n",
    ")\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we load the sycamore quantum circuit with $n=30$, $m=14$ and EFGH sequence. `sc_target=30` means the largest size of intermediate tensors is $2^{30}$ (since the data type is `complex64`, it will take about 8G of memory). Thus, in order to perform such a contraction, you need a GPU with memory larger than 24G (`einsum` operator in pytorch need to take 3 times of involving tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m, seq, device, sc_target, seed = 30, 14, 'EFGH', 'cuda', 30, 0\n",
    "qc = QuantumCircuit(n, m, seq=seq)\n",
    "edges = []\n",
    "for i in range(len(qc.neighbors)):\n",
    "    for j in qc.neighbors[i]:\n",
    "        if i < j:\n",
    "            edges.append((i, j))\n",
    "neighbors = list(qc.neighbors)\n",
    "final_qubits = set(range(len(neighbors) - n, len(neighbors)))\n",
    "tensor_bonds = [[edges.index((min(i, j), max(i, j))) for j in neighbors[i]] for i in range(len(neighbors)) if i not in final_qubits] # open tensor network without final state\n",
    "bond_dims = {i:2.0 for i in range(len(edges))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contract the open tensor network corresponding to the quantum circuit, to get the overall $2^{30}$ amplitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_slicing = [(1, 11), (1, 3), (0, 24), (1, 12), (0, 36), (27, 40), (2, 10), (1, 13), (0, 23), (28, 38), (7, 18), (5, 29), (41, 76), (27, 52), (1, 2), (22, 35), (0, 34), (60, 71), (7, 28), (5, 17), (27, 41), (26, 37), (1, 22), (0, 60), (5, 7), (26, 27), (25, 49), (14, 39), (6, 15), (4, 16), (0, 1), (5, 26), (48, 59), (50, 58), (14, 25), (4, 6), (0, 5), (48, 50), (4, 14), (0, 48), (65, 75), (51, 57), (0, 4), (65, 85), (33, 43), (32, 45), (64, 80), (9, 20), (8, 21), (0, 51), (65, 88), (32, 33), (54, 64), (8, 9), (66, 77), (30, 44), (42, 61), (62, 72), (53, 63), (0, 65), (32, 54), (46, 56), (8, 19), (30, 66), (31, 42), (53, 62), (0, 32), (8, 46), (30, 31), (0, 53), (47, 69), (73, 84), (8, 30), (0, 47), (74, 87), (73, 96), (0, 8), (73, 74), (55, 68), (0, 73), (81, 99), (0, 55), (70, 81), (0, 70), (82, 104), (83, 107), (0, 82), (67, 83), (79, 92), (86, 98), (95, 106), (97, 105), (0, 67), (79, 86), (101, 111), (95, 97), (0, 79), (90, 101), (0, 95), (90, 121), (78, 89), (100, 110), (0, 90), (78, 100), (112, 122), (109, 119), (94, 117), (128, 141), (0, 78), (109, 112), (93, 120), (116, 129), (94, 130), (118, 128), (0, 109), (93, 103), (116, 142), (94, 118), (91, 113), (0, 93), (94, 116), (91, 102), (0, 94), (91, 115), (123, 132), (0, 91), (114, 125), (108, 133), (0, 123), (114, 140), (108, 124), (0, 114), (134, 145), (127, 137), (126, 139), (0, 108), (134, 138), (131, 143), (126, 127), (0, 134), (131, 144), (126, 148), (0, 131), (135, 146), (136, 147), (0, 126), (135, 149), (136, 150), (0, 135), (0, 136)]\n",
      "slicing_bonds = {}\n"
     ]
    }
   ],
   "source": [
    "order_slicing, slicing_bonds, ctree_new = find_order(tensor_bonds, bond_dims, seed, sc_target=sc_target, trials=5, iters=10, slicing_repeat=1, betas=np.linspace(3.0, 21.0, 61))\n",
    "print('order_slicing =', order_slicing)\n",
    "print('slicing_bonds =', slicing_bonds)\n",
    "\n",
    "tensors = []\n",
    "for x in range(len(qc.tensors)):\n",
    "    if x not in final_qubits:\n",
    "        tensors.append(qc.tensors[x].to(device))\n",
    "\n",
    "scheme, bonds_final, _ = contraction_scheme_sparse_einsum_1(ctree_new, [], sc_target=sc_target)\n",
    "\n",
    "final_qubits = sorted(final_qubits)\n",
    "permute_dims = [0] * len(final_qubits)\n",
    "for x in range(len(bonds_final)):\n",
    "    _, y = edges[bonds_final[x]]\n",
    "    permute_dims[list(final_qubits).index(y)] = x\n",
    "full_amps = tensor_contraction_einsum(deepcopy(tensors), scheme).permute(permute_dims).reshape(-1).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of the relation between slicing edges and fidelity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after slicing 1 edges, fidelity now is 0.49377 (estimated value 0.5)\n",
      "after slicing 2 edges, fidelity now is 0.26449 (estimated value 0.25)\n",
      "after slicing 3 edges, fidelity now is 0.13281 (estimated value 0.125)\n",
      "after slicing 4 edges, fidelity now is 0.06634 (estimated value 0.0625)\n",
      "after slicing 5 edges, fidelity now is 0.03349 (estimated value 0.03125)\n",
      "after slicing 6 edges, fidelity now is 0.01618 (estimated value 0.015625)\n",
      "after slicing 7 edges, fidelity now is 0.00607 (estimated value 0.0078125)\n",
      "after slicing 8 edges, fidelity now is 0.00304 (estimated value 0.00390625)\n"
     ]
    }
   ],
   "source": [
    "slicing_edges_manually_select = [(44, 66), (46, 66), (69, 94), (81, 94), (88, 99), (94, 116), (111, 127), (112, 122),]\n",
    "\n",
    "tensors_slicing = deepcopy(tensors)\n",
    "slicing_indices = {}\n",
    "neighbors_copy = deepcopy(neighbors)\n",
    "tensor_network = AbstractTensorNetwork(\n",
    "    deepcopy(tensor_bonds), \n",
    "    deepcopy(bond_dims))\n",
    "\n",
    "while len(slicing_edges_manually_select):\n",
    "    slicing_edge = slicing_edges_manually_select.pop(0)\n",
    "    x, y = slicing_edge\n",
    "    idx_x_y = neighbors_copy[x].index(y)\n",
    "    idx_y_x = neighbors_copy[y].index(x)\n",
    "    neighbors_copy[x].pop(idx_x_y)\n",
    "    neighbors_copy[y].pop(idx_y_x)\n",
    "    slicing_indices[(x, y)] = (idx_x_y, idx_y_x)\n",
    "    tensors_slicing[x] = tensors_slicing[x].select(idx_x_y, 0)\n",
    "    tensors_slicing[y] = tensors_slicing[y].select(idx_y_x, 0)\n",
    "\n",
    "    tensor_network.slicing(edges.index(slicing_edge))\n",
    "    ctree_appro = ContractionTree(deepcopy(tensor_network), order_slicing, 0)\n",
    "    scheme, _, _ = contraction_scheme_sparse_einsum_1(ctree_appro, [], sc_target=sc_target)\n",
    "    appro_amps = tensor_contraction_einsum(deepcopy(tensors_slicing), scheme).permute(permute_dims).reshape(-1).cpu()\n",
    "    fidelity = ((full_amps.conj() @ appro_amps.reshape(-1)).abs() / \\\n",
    "        (full_amps.abs().square().sum().sqrt() * appro_amps.abs().square().sum().sqrt())).square().item()\n",
    "    \n",
    "    print(f'after slicing {len(slicing_indices)} edges, fidelity now is {fidelity:.5f} (estimated value {1/2**(len(slicing_indices))})')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the amplitudes calculated by Google using Schrodinger-Feynman algorithm as the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_samples(filename):\n",
    "    import os\n",
    "    if os.path.exists(filename):\n",
    "        samples_data = []\n",
    "        with open(filename, 'r') as f:\n",
    "            l = f.readlines()\n",
    "        f.close()\n",
    "        for line in l:\n",
    "            ll = line.split()\n",
    "            samples_data.append((ll[0], float(ll[1]) + 1j*float(ll[2])))\n",
    "        return samples_data\n",
    "    else:\n",
    "        raise ValueError(\"{} does not exist\".format(filename))\n",
    "\n",
    "data = read_samples('amplitudes_n30_m14_s0_e0_pEFGH_10000.txt')\n",
    "max_bitstrings = 1_000\n",
    "bitstrings = [data[i][0] for i in range(max_bitstrings)]\n",
    "amplitude_google = np.array([data[i][1] for i in range(max_bitstrings)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that the result calculated by sparse-state is identical to Goggle's result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitstring amplitude correct ratio:0.995\n"
     ]
    }
   ],
   "source": [
    "tensors_sparsestate = []\n",
    "for i in range(len(qc.tensors)):\n",
    "    if i in final_qubits:\n",
    "        tensors_sparsestate.append(\n",
    "            torch.tensor([[1, 0], [0, 1]], dtype=torch.complex64, device=device)\n",
    "        )\n",
    "    else:\n",
    "        tensors_sparsestate.append(qc.tensors[i].to(device))\n",
    "\n",
    "tensor_bonds = [[edges.index((min(i, j), max(i, j))) for j in neighbors[i]] for i in range(len(neighbors))] # now all tensors will be included\n",
    "\n",
    "order_slicing, slicing_bonds, ctree_new = find_order(\n",
    "    tensor_bonds, bond_dims, seed, final_qubits, max_bitstrings, sc_target=sc_target, trials=5, iters=10, slicing_repeat=1, betas=np.linspace(3.0, 21.0, 61)\n",
    ")\n",
    "\n",
    "scheme_sparsestate, _, bitstrings_sorted = contraction_scheme_sparse_einsum_1(ctree_new, bitstrings, sc_target=sc_target)\n",
    "\n",
    "slicing_edges = [edges[i] for i in slicing_bonds]\n",
    "slicing_indices = {}.fromkeys(slicing_edges)\n",
    "neighbors_copy = deepcopy(neighbors)\n",
    "for i, j in slicing_edges:\n",
    "    idxi_j = neighbors_copy[i].index(j)\n",
    "    idxj_i = neighbors_copy[j].index(i)\n",
    "    neighbors_copy[i].pop(idxi_j)\n",
    "    neighbors_copy[j].pop(idxj_i)\n",
    "    slicing_indices[(i, j)] = (idxi_j, idxj_i)\n",
    "\n",
    "\n",
    "amplitude_sparsestate = torch.zeros((len(bitstrings),), dtype=torch.complex64, device=device)\n",
    "for s in range(2**len(slicing_edges)):\n",
    "    configs = list(map(int, np.binary_repr(s, len(slicing_edges))))\n",
    "    sliced_tensors = tensors_sparsestate.copy()\n",
    "    for i in range(len(slicing_edges)):\n",
    "        m, n = slicing_edges[i]\n",
    "        idxm_n, idxn_m = slicing_indices[(m, n)]\n",
    "        sliced_tensors[m] = sliced_tensors[m].select(idxm_n, configs[i]).clone()\n",
    "        sliced_tensors[n] = sliced_tensors[n].select(idxn_m, configs[i]).clone()\n",
    "    amplitude_sparsestate += tensor_contraction_einsum(sliced_tensors, scheme_sparsestate)\n",
    "\n",
    "correct_num = 0\n",
    "for i in range(len(bitstrings_sorted)):\n",
    "    ind_google = bitstrings.index(bitstrings_sorted[i])\n",
    "    relative_error = abs(amplitude_sparsestate[i].item() - amplitude_google[ind_google]) / abs(amplitude_google[ind_google])\n",
    "    if relative_error <= 0.05:\n",
    "        correct_num += 1\n",
    "print(f'bitstring amplitude correct ratio:{correct_num/max_bitstrings}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f95100793b43010e28ed80d490b665ccbe9af4299329cffb7252da6f38962fd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
